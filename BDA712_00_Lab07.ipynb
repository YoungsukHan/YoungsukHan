{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Session \\# 07\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "by Josué Obregón <br>\n",
        "BDA712-00 - Machine Learning Programming <br>\n",
        "Department of Big Data Analytics - Kyung Hee University<br>\n",
        "\n",
        "## Objective\n",
        "\n",
        "The objective of this session is learn how to implement a one layer neural network."
      ],
      "metadata": {
        "id": "jCqG2Vz70NNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the data"
      ],
      "metadata": {
        "id": "tffQN5TXQoMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "2M5n_qs5acMe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "vouLYEd0aZcf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = ['https://drive.google.com/uc?export=download&id=1MnUH3W1Jm8LVBqEJ1M0m5l9_Q8hJZqrz', # train-labels-idx1-ubyte.gz  https://drive.google.com/file/d/1MnUH3W1Jm8LVBqEJ1M0m5l9_Q8hJZqrz/view?usp=sharing       \n",
        "        'https://drive.google.com/uc?export=download&id=1AZLWnMx1xe3vN1naEswKL19I02YrA7_J', # train-images-idx3-ubyte.gz  https://drive.google.com/file/d/1AZLWnMx1xe3vN1naEswKL19I02YrA7_J/view?usp=sharing       \n",
        "        'https://drive.google.com/uc?export=download&id=1Hw8QHRxmI4w-ZAo5yzVjDB3UnUPAVv4u', # t10k-labels-idx1-ubyte.gz  https://drive.google.com/file/d/1Hw8QHRxmI4w-ZAo5yzVjDB3UnUPAVv4u/view?usp=sharing       \n",
        "        'https://drive.google.com/uc?export=download&id=1EHdJfVQs1ZiRhCoEldMc9lTJ-5Nz5GaV', # t10k-images-idx3-ubyte.gz  https://drive.google.com/file/d/1EHdJfVQs1ZiRhCoEldMc9lTJ-5Nz5GaV/view?usp=sharing       \n",
        "      ]\n",
        "outputs = ['train-labels-idx1-ubyte.gz', 'train-images-idx3-ubyte.gz',\n",
        "           't10k-labels-idx1-ubyte.gz', 't10k-images-idx3-ubyte.gz']\n",
        "for url,output in zip(urls,outputs):\n",
        "  gdown.download(url, f'data/{output}', quiet=False)"
      ],
      "metadata": {
        "id": "gkFTKhLl0I72",
        "outputId": "fc90b670-cd97-4cfb-e6b6-ea95d11b4ea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1MnUH3W1Jm8LVBqEJ1M0m5l9_Q8hJZqrz\n",
            "To: /content/data/train-labels-idx1-ubyte.gz\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 26.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1AZLWnMx1xe3vN1naEswKL19I02YrA7_J\n",
            "To: /content/data/train-images-idx3-ubyte.gz\n",
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 49.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1Hw8QHRxmI4w-ZAo5yzVjDB3UnUPAVv4u\n",
            "To: /content/data/t10k-labels-idx1-ubyte.gz\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.46MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1EHdJfVQs1ZiRhCoEldMc9lTJ-5Nz5GaV\n",
            "To: /content/data/t10k-images-idx3-ubyte.gz\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 139MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries\n",
        "\n",
        "Let's import the data and prepare the variables that we will need for our laboratory"
      ],
      "metadata": {
        "id": "zhc1fdmralXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "import struct\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ElL0PnEca2tJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for loading images and labels of MNIST dataset"
      ],
      "metadata": {
        "id": "NHsepsXCE6g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "import struct\n",
        "\n",
        "\n",
        "def load_images(filename):\n",
        "    # Open and unzip the file of images:\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        # Read the header information into a bunch of variables\n",
        "        _ignored, n_images, columns, rows = struct.unpack('>IIII', f.read(16))\n",
        "        # Read all the pixels into a NumPy array of bytes:\n",
        "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "        # Reshape the pixels into a matrix where each line is an image:\n",
        "        return all_pixels.reshape(n_images, columns * rows)\n",
        "\n",
        "def load_labels(filename):\n",
        "    # Open and unzip the file of images:\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        # Skip the header bytes:\n",
        "        f.read(8)\n",
        "        # Read all the labels into a list:\n",
        "        all_labels = f.read()\n",
        "        # Reshape the list of labels into a one-column matrix:\n",
        "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "RScsBDxmm7id"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our `one_hot_encode` and `prepend_bias` function"
      ],
      "metadata": {
        "id": "O1fIc0ukFTkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(Y):\n",
        "  unique_labels =  np.unique(Y)\n",
        "  return (Y == unique_labels).astype(int)"
      ],
      "metadata": {
        "id": "atB0IfopFmHx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepend_bias(X):\n",
        "    # Insert a column of 1s in the position 0 of X.\n",
        "    # (“axis=1” stands for: “insert a column, not a row”)\n",
        "  return np.insert(X, 0, 1, axis=1) # insert(arr, obj, values, axis=None)"
      ],
      "metadata": {
        "id": "2IWFEDb1E1xg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load our data into our main variables"
      ],
      "metadata": {
        "id": "zrcDO7GiF_Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 60000 images, each 784 elements (28 * 28 pixels)\n",
        "X_train = load_images(\"data/train-images-idx3-ubyte.gz\")\n",
        "\n",
        "# 10000 images, each 784 elements, with the same structure as X_train\n",
        "X_test = load_images(\"data/t10k-images-idx3-ubyte.gz\")\n",
        "\n",
        "# 60K labels, each a single digit from 0 to 9\n",
        "Y_train_unencoded = load_labels(\"data/train-labels-idx1-ubyte.gz\")\n",
        "\n",
        "# 60K labels, each consisting of 10 one-hot encoded elements\n",
        "Y_train = one_hot_encode(Y_train_unencoded)\n",
        "\n",
        "# 10000 labels, each a single digit from 0 to 9\n",
        "Y_test = load_labels(\"data/t10k-labels-idx1-ubyte.gz\")"
      ],
      "metadata": {
        "id": "hv3YmTIzn6mg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training data shapes: ')\n",
        "print(X_train.shape)\n",
        "print(Y_train_unencoded.shape)\n",
        "print(Y_train.shape)\n",
        "print('Test data shapes: ')\n",
        "print(Y_test.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "hqpRX6ulGfNl",
        "outputId": "5a2939ab-52d5-4fa6-9665-04b548c0782c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shapes: \n",
            "(60000, 784)\n",
            "(60000, 1)\n",
            "(60000, 10)\n",
            "Test data shapes: \n",
            "(10000, 1)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Forward Propagation"
      ],
      "metadata": {
        "id": "PvMQ5i48sMyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a reprint of `forward`, a core function of our perceptron:\n",
        "\n",
        "```python\n",
        "def forward(X, w):\n",
        "  weighted_sum = np.matmul(X, w)\n",
        "  return sigmoid(weighted_sum)\n",
        "```"
      ],
      "metadata": {
        "id": "JeZtJA_ZHBbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`forward()` implements the operation that we called “forward propagation”:\n",
        "* It calculates the system’s outputs from the system’s inputs. \n",
        "\n",
        "In the case of the perceptron, it applies a weighted sum followed by a sigmoid. In the case of a neural network, things become slightly more complicated."
      ],
      "metadata": {
        "id": "TCs3W7xuHSwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by defining our activations functions: sigmoid and softmax.\n",
        "\n",
        "  $sigmoid(z)=\\frac{1}{1+e^{-z}}$\n",
        "\n",
        "  $softmax(l_i)=\\frac{e^{l_i}}{∑ e^{l}}$\n"
      ],
      "metadata": {
        "id": "XICjnjQoxmXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "-jT39E21HSHZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "  exponentials = np.exp(logits)\n",
        "  denominator = np.sum(exponentials, axis=1).reshape(-1,1)\n",
        "  return exponentials / denominator"
      ],
      "metadata": {
        "id": "-DZ1ZNpNy4j8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFkneRAzxMSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the new softmax function. (We already tried the sigmoid function before)"
      ],
      "metadata": {
        "id": "Zg5KFh3-0wAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[0.3, 0.8, 0.2],\n",
        "             [0.1, 0.9, 0.1]])"
      ],
      "metadata": {
        "id": "Ui58aQnN0vJa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(a)"
      ],
      "metadata": {
        "id": "KTKvut8k1KWz",
        "outputId": "54ec26bb-9e21-4920-a367-c4263a4587fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.28140804, 0.46396343, 0.25462853],\n",
              "       [0.23665609, 0.52668782, 0.23665609]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happen if we want to use only one sample?"
      ],
      "metadata": {
        "id": "L6rmxX0U74kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(a[0])"
      ],
      "metadata": {
        "id": "I2zoMmzQ1exk",
        "outputId": "a7b2e426-736b-4cdf-c9c1-ecf634ddf3e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AxisError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-dec2cce75328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-9e9a23b05e0a>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(logits)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mexponentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexponentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexponentials\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2260\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(a[0].reshape(1, -1))"
      ],
      "metadata": {
        "id": "n09_2vre1N9V",
        "outputId": "17df2c7b-71b7-47ec-9ca7-34ca58eeef7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.28140804, 0.46396343, 0.25462853]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happen if our probabilities divierge a lot?"
      ],
      "metadata": {
        "id": "hcfdvs7t8pzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = np.array([1,20]).reshape(1, -1)\n",
        "c = np.array([1,1000]).reshape(1, -1)"
      ],
      "metadata": {
        "id": "q_xWLml18m4K"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(b)"
      ],
      "metadata": {
        "id": "QnGV_59484wW",
        "outputId": "a7e5e224-1f8d-4798-d7fc-6096a815e01f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.60279641e-09, 9.99999994e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(c)"
      ],
      "metadata": {
        "id": "8WrA0f5w84gW",
        "outputId": "1d6a8383-9914-4775-b82e-f01bb2fdd0e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0., nan]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our implementations of `softmax()` have a problem: it is numerically unstable, meaning that they amplify small changes in the inputs.\n",
        "\n",
        "In other woreds, `softmax()` function is prone to two issues: **overflow** and **underflow**\n",
        "\n",
        "**Overflow**: It occurs when very large numbers are approximated as infinity\n",
        "\n",
        "**Underflow**: It occurs when very small numbers (near zero in the number line) are approximated (i.e. rounded to) as zero\n",
        "\n",
        "To combat these issues when doing softmax computation, a common trick is to shift the input vector by subtracting the maximum element in it from all elements. For the input vector *logits*, define *z* such that:\n",
        "\n",
        "```python\n",
        "z = logits - np.max(logits, axis=1).reshape(-1,1)\n",
        "```\n",
        "And then we take the softmax fo the new (stable) vector *z*."
      ],
      "metadata": {
        "id": "GTpTRwWrD1LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stable_softmax(logits):\n",
        "    z = logits - np.max(logits, axis=1).reshape(-1,1)\n",
        "    numerator = np.exp(z)\n",
        "    denominator = np.sum(numerator,axis=1).reshape(-1,1)\n",
        "    return numerator/denominator"
      ],
      "metadata": {
        "id": "Q6-9GVzEGWH8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check that everything is still working correctly"
      ],
      "metadata": {
        "id": "VU_EH2TKEtQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stable_softmax(c)"
      ],
      "metadata": {
        "id": "DcLZRhBwErbG",
        "outputId": "e5b49e70-7211-44a3-c9cd-52e72ac2cba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stable_softmax(a)"
      ],
      "metadata": {
        "id": "2Bwm-EcWGddU",
        "outputId": "26a3623e-89cb-42a9-a8b6-4b3b5247ef0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.28140804, 0.46396343, 0.25462853],\n",
              "       [0.23665609, 0.52668782, 0.23665609]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stable_softmax(b)"
      ],
      "metadata": {
        "id": "O7G_g2ZrEpjZ",
        "outputId": "44929c96-fffe-4d39-cb3c-83e0a6f22af2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.60279641e-09, 9.99999994e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(a)"
      ],
      "metadata": {
        "id": "nbSEfMbNG07x",
        "outputId": "8848b87c-6f82-4f91-c696-f5e903e79493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.28140804, 0.46396343, 0.25462853],\n",
              "       [0.23665609, 0.52668782, 0.23665609]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(b)"
      ],
      "metadata": {
        "id": "3NMZTF7CEx9u",
        "outputId": "b4b63018-d558-4847-be87-263ea63876cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.60279641e-09, 9.99999994e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(c)"
      ],
      "metadata": {
        "id": "68jjl3hvEyMN",
        "outputId": "9914e52f-3321-4b8d-89bd-c5a9fcab9a44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0., nan]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very well, after defining our activation functions, Let's move to implement the forward function.\n",
        "\n",
        "The first step of forward propagation is the same as a regular perceptron’s:"
      ],
      "metadata": {
        "id": "sBy_blF7zNRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X, w1, w2):\n",
        "  h = sigmoid(np.matmul(prepend_bias(X), w1))\n",
        "  y_hat = stable_softmax(np.matmul(prepend_bias(h), w2))\n",
        "  return y_hat, h"
      ],
      "metadata": {
        "id": "71g2XVKezMPx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "N:ow we have the `forward()` function, and both activation functions—the sigmoid and the softmax. \n",
        "\n",
        "To complete the classifier, however, we still need to take care of the higher-level classification and testing function: `predict()` and `report()`.\n",
        "\n",
        "Let's remember our previous implementations of predict function.\n",
        "\n",
        "```python\n",
        "def predict(X, beta, return_proba=False):\n",
        "  y_hat =  forward(X,beta)\n",
        "  if return_proba:\n",
        "    return y_hat\n",
        "  else:\n",
        "    labels = np.argmax(y_hat, axis=1)\n",
        "    return labels.reshape(-1,1)\n",
        "```"
      ],
      "metadata": {
        "id": "lGrIqWdY2Zfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w1, w2, return_proba=False):\n",
        "  y_hat, _ =  forward(X, w1, w2)\n",
        "  if return_proba:\n",
        "    return y_hat\n",
        "  else:\n",
        "    labels = np.argmax(y_hat, axis=1)\n",
        "    return labels.reshape(-1,1)"
      ],
      "metadata": {
        "id": "97C3ib4zzLj7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to create a function called `report` for measuring and reporting the performance of our model (in terms of accuracy) during our training process. \n",
        "\n",
        "The function will give information during each step of our Gradient Descent process. It will do the following:\n",
        "\n",
        "* Obtain the estimations $\\hat{y}$ using the training data\n",
        "* Compute the loss \n",
        "* Get the predictions (classifications)\n",
        "* Compute the accuracy\n",
        "* Report everything"
      ],
      "metadata": {
        "id": "cknDrTBB5cmV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6isFBup48IMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def report(iteration, X_train, Y_train, X_test, Y_test, w1, w2):\n",
        "  y_hat = forward(X_train, w1, w2)\n",
        "  # y_hat = predict(X_train, w1, w2, return_proba=False)\n",
        "  training_loss = loss(Y_train, y_hat)\n",
        "  classifications = predict(X_test, w1, w2)\n",
        "  accuracy = np.average(classifications==Y_test) * 100\n",
        "  print(f'Iteration : {iteration} Loss : {training_loss:.6f} Accuracy : {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "g5t_A2PX5bgd"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A new loss function"
      ],
      "metadata": {
        "id": "k_awFayUKvEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we used the log loss formula for our binary classifiers. We even used the log loss when we bundled ten binary classifiers in a multiclass classifier.\n",
        "\n",
        "In that case, we added together the losses of the ten classifiers to get a total loss."
      ],
      "metadata": {
        "id": "psuXYlbdK2UI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the log loss served us well so far, it’s time to switch to a simpler formula\n",
        "—one that’s specific to multiclass classifiers. It’s called the cross-entropy loss,\n",
        "and it looks like this:\n",
        "\n",
        "$L(y,\\hat{y})=\\frac{1}{n} \\sum_{i=1}^{n} y^{(i)} \\cdot log(\\hat{y}^{(i)}) $"
      ],
      "metadata": {
        "id": "GRcaj0DTRuRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(Y, y_hat):\n",
        "  # print(Y.shape)\n",
        "  # print(y_hat)\n",
        "  print(y_hat.shape)\n",
        "  return -np.sum(Y * np.log(y_hat)) / Y.shape[0]"
      ],
      "metadata": {
        "id": "m8n0vxd0KuQS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a pragmatic reason to use the cross-entropy loss in our neural network: it’s a perfect match for the softmax. More specifically, a softmax followed by a cross-entropy loss makes it easier to code gradient descent."
      ],
      "metadata": {
        "id": "LFnqgPhmUU78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the 'loss()' function, half of our neural network’s logic is done—the half that takes care\n",
        "of the `classification` phase.\n",
        "\n",
        "This code, however, won’t be useful until we’ve also written the code that trains the network. \n",
        "\n",
        "Let's move to that!\n",
        "\n"
      ],
      "metadata": {
        "id": "YE-r327SUkCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation\n",
        "\n"
      ],
      "metadata": {
        "id": "czahv9KesRNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After learning about backpropagation on the slides, let's implement that function. \n",
        "\n",
        "Let's start by definining the gradient of the sigmoid function\n",
        "\n",
        "$\\sigma^{'}=\\sigma (1-\\sigma)$"
      ],
      "metadata": {
        "id": "y4Wam92iPG32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_gradient(sigmoid):\n",
        "  return np.multiply(sigmoid, (1-sigmoid))"
      ],
      "metadata": {
        "id": "iZLQCe1wPGJy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define the backpropagation function:\n",
        "\n",
        "* $\\frac{\\partial{L}}{\\partial{w_1}}=(\\hat{y}-y)\\cdot h$\n",
        "* $\\frac{\\partial{L}}{\\partial{w_2}}=(\\hat{y}-y)\\cdot w_2 \\cdot \\sigma \\cdot (1-\\sigma) \\cdot x$"
      ],
      "metadata": {
        "id": "x6bsCs2cQaXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back(X, Y, y_hat, w2, h):\n",
        "  w2_gradient = np.matmul(prepend_bias(h).T, (y_hat - Y)) / X.shape[0]\n",
        "  w1_gradient = np.matmul(prepend_bias(X).T, np.matmul((y_hat - Y), w2[1:].T)*sigmoid_gradient(h)) / X.shape[0]\n",
        "  return (w1_gradient, w2_gradient)"
      ],
      "metadata": {
        "id": "VeIO9ZRBRdCN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Las step, initializing our weights"
      ],
      "metadata": {
        "id": "o5vu3NiwXsLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we just discussed. We should initialize our weights randomly.\n",
        "\n",
        "How lare or how small those values should be?\n",
        "\n",
        "Let's check something..."
      ],
      "metadata": {
        "id": "FVUaQqh4YCez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_sum = 1000 * 10\n",
        "sigmoid(weighted_sum)"
      ],
      "metadata": {
        "id": "oRQyMWKWYBxC",
        "outputId": "65e3c4e8-7e88-4554-8f9a-75f99b8f21cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid_gradient(sigmoid(weighted_sum))"
      ],
      "metadata": {
        "id": "roCgP39gXriF",
        "outputId": "c396abe0-f0bd-41a8-9679-7f0399db6ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4-3eau_QYesg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember the chain rule? During backpropagation, this gradient of 0 gets\n",
        "multiplied by the other local gradients, causing the entire gradient to become 0. With a gradient of 0, gradient descent has nowhere to go."
      ],
      "metadata": {
        "id": "Dpfa6eY-Yo7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s wrap up what we said in the previous sections. We should initialize\n",
        "weights with values that are:\n",
        "* Random (to break symmetry)\n",
        "* Small (to speed up training and avoid dead neurons)\n",
        "\n",
        "\n",
        "Let's use an empirical rule of thumb. We’ll make each weight range from 0 to\n",
        "something around the following value, where $r$ is the number of rows in the weight matrix:\n",
        "\n",
        "$w \\approx \\pm \\sqrt{\\frac{1}{r}} $\n",
        "\n",
        "Let's implement our function:"
      ],
      "metadata": {
        "id": "RG-znfSoYu5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(n_input_variables, n_hidden_nodes, n_classes):\n",
        "  w1_rows = n_input_variables + 1\n",
        "  w1 = np.random.randn(w1_rows, n_hidden_nodes) * np.sqrt(1/w1_rows)\n",
        "  w2_rows = n_hidden_nodes + 1\n",
        "  w2 = np.random.randn(w2_rows, n_classes) * np.sqrt(1/w2_rows)\n",
        "  return w1, w2"
      ],
      "metadata": {
        "id": "U_bmVjBeYoXV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that each matrix of weights\n",
        "in a neural network has as many rows as its input elements, and as many\n",
        "columns as its output elements.\n",
        "\n",
        "Note: If you initialize a neural network with random weights, then you should expect a\n",
        "slightly different loss every time you train it. To avoid that randomess, let's seed NumPy’s random number generator with `np.random.seed(a_known_seed)`."
      ],
      "metadata": {
        "id": "7TXkWR9navl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(712)"
      ],
      "metadata": {
        "id": "37e5KlL-a6EF"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything is finished now. Let's create our train function and let's try it!!!"
      ],
      "metadata": {
        "id": "mqJEIfoJbQKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train, Y_train, X_test, Y_test, n_hidden_nodes, iterations, lr):\n",
        "    n_input_variables = X_train.shape[1]\n",
        "    n_classes = Y_train.shape[1]\n",
        "    w1, w2 = initialize_weights(n_input_variables, n_hidden_nodes, n_classes)\n",
        "    for iteration in range(iterations):\n",
        "        y_hat, h = forward(X_train, w1, w2)\n",
        "        w1_gradient, w2_gradient = back(X_train, Y_train, y_hat, w2, h)\n",
        "        w1 = w1 - (w1_gradient * lr)\n",
        "        w2 = w2 - (w2_gradient * lr)\n",
        "        report(iteration, X_train, Y_train, X_test, Y_test, w1, w2)\n",
        "    return w1, w2"
      ],
      "metadata": {
        "id": "S-EhLLi9ausm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "w1, w2 = train(X_train, Y_train, X_test, Y_test, n_hidden_nodes=200, iterations = 20, lr=0.01)"
      ],
      "metadata": {
        "id": "LOuNdysPbjt9",
        "outputId": "028051b9-b806-4c82-a7df-24e925210df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-5d38767487ba>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train, Y_train, X_test, Y_test, n_hidden_nodes, iterations, lr)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw1_gradient\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw2_gradient\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-9f8e233f3446>\u001b[0m in \u001b[0;36mreport\u001b[0;34m(iteration, X_train, Y_train, X_test, Y_test, w1, w2)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# y_hat = predict(X_train, w1, w2, return_proba=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mclassifications\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifications\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-40d5579079e9>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(Y, y_hat)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# print(Y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# print(y_hat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QQzVgBytbkFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4wqNvNlbkTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary \n",
        "\n",
        "Let’s recap what we learned today:\n",
        "\n",
        "* In this lab session, you learned backpropagation—an algorithm to calculate the\n",
        "gradients of the weights in a neural network.\n",
        "\n",
        "* Those gradients represent the\n",
        "impact of each weight on the overall loss.\n",
        "Each training iteration in a neural network ping-pongs between two steps:\n",
        "  1. **Forward propagation**: the network calculates each layer from the previous\n",
        "one, from the input layer to the output ŷ.\n",
        "  2. **Backpropagation**: the network bounces its way back from the output layer\n",
        "to the weights, using the chain rule to calculate their gradients. Then it\n",
        "descends those gradients, pushing the loss down, and $\\hat{y}$ closer to the ground truth $y$.\n",
        "\n",
        "* You also learned that neural networks don’t train well if all the weights have\n",
        "the same value.\n",
        "  * Instead, you should break their symmetry by initializing the weights with random values. \n",
        "  *Those initial values should also be small to avoid problems like overflows and dead neurons."
      ],
      "metadata": {
        "id": "Ru5EAADvQJi-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwvsEvDx0Z01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}